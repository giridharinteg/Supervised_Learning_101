{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "- End of the day we are looking to have a model which is best representing the data.\n",
    "- Get the best model\n",
    "- In Linear reg -> We are just choosing parameters that best represent the model\n",
    "- In Lasso/Ridge we try to vary alpha\n",
    "- In K-nn We want to choose n_neighbors neighbors\n",
    "\n",
    "- These values like alpha and k are called as hyper parameters\n",
    "\n",
    "- Now comes the art of choosing the right values for the huperparametrs\n",
    "\n",
    "- as you see these values can't be learned\n",
    "\n",
    "### Choosing the right values\n",
    "\n",
    "- We can try different values\n",
    "\n",
    "- Fit them seperately\n",
    "\n",
    "- see how well each performs\n",
    "\n",
    "- see which is best\n",
    "\n",
    "- It is essential to use CV here\n",
    "\n",
    "This process is called as hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have two hyperparameters \n",
    "C and K\n",
    "then we use something called as a grid search CV\n",
    "\n",
    "![Grid Search CV](grid search cv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use this\n",
    "from sklearn.model_seletion import GridSearchCV\n",
    "param_grid = {'n_neighbors':np.arange(1. 50)} # dictionary of hyperparameter to value range pair\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv = 5)\n",
    "knn_cv.fit(X, y)\n",
    "knn_cv.best_params_\n",
    "knn_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "Lets tune the n_neighbors parameter of the KNeighborsClassifier() <br>\n",
    "using GridSearchCV on logistic regression on the diabetes dataset instead! <br><br>\n",
    "\n",
    "Like the alpha parameter of lasso and ridge regularization that you saw earlier, <br>\n",
    "logistic regression also has a regularization parameter: C. <br>\n",
    "    C controls the inverse of the regularization strength, and this is what you will tune in this exercise. <br>\n",
    "    A large C can lead to an overfit model, while a small C can lead to an underfit model. <br><br>\n",
    "\n",
    "Lets use GridSearchCV and logistic regression to find the optimal C in this hyperparameter space. <br>\n",
    "\n",
    "Here, we are focusing on the process of setting up the hyperparameter grid and performing grid-search cross-validation. <br>\n",
    "In practice, you will indeed want to hold out a portion of your data for evaluation purposes, <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('/media/Datascience/Projects/Giridhar/Datasets/pima-indians-diabetes-database/diabetes.csv')\n",
    "X = df.iloc[:,range(0,8)].values\n",
    "y = df.iloc[:,[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1)\n",
      "Tuned Logistic Regression Parameters: {'C': 268.2695795279727}\n",
      "Best score is 0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "print(y.shape)\n",
    "y_mod = np.ravel(y,order='C')\n",
    "logreg_cv.fit(X, y_mod)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with RandomizedSearchCV\n",
    "GridSearchCV can be computationally expensive, <br>\n",
    "especially if you are searching over a large hyperparameter space <br>\n",
    "and dealing with multiple hyperparameters. <br>\n",
    "\n",
    "A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. <br>\n",
    "Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be using a new model: the Decision Tree. \n",
    "    Don't worry about the specifics of how this model works. \n",
    "    Just like k-NN, linear regression, and logistic regression, \n",
    "    decision trees in scikit-learn have .fit() and .predict() methods \n",
    "    that you can use in exactly the same way as before. \n",
    "    \n",
    "    Decision trees have many parameters that can be tuned, \n",
    "    such as max_features, max_depth, and min_samples_leaf.\n",
    "    This makes it an ideal use case for RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 7, 'min_samples_leaf': 7}\n",
      "Best score is 0.7356770833333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time.\n",
    "\n",
    "### Hold Out set - Need\n",
    "\n",
    "- A Hold out set is a set of data held for testing\n",
    "- It will not be used in the CV\n",
    "- Thus this would give us a clear idea on how the model performs on unseen data\n",
    "\n",
    "\n",
    "In addition to C, logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization.\n",
    "\n",
    "Lets create a hold-out set, tune the 'C' and 'penalty' hyperparameters of a logistic regression classifier using GridSearchCV on the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Logistic Regression Parameter: {'C': 31.622776601683793, 'penalty': 'l2'}\n",
      "Tuned Logistic Regression Accuracy: 0.7673913043478261\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv = 5)\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out set in practice II: Regression\n",
    "\n",
    "Remember lasso and ridge regression from the previous chapter? \n",
    "Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. \n",
    "There is another type of regularized regression known as the elastic net.\n",
    "In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n",
    "<br>\n",
    "<hr> a∗L1+b∗L2 <hr>\n",
    "In scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an L1 penalty, and anything lower is a combination of L1 and L2.\n",
    "\n",
    "In this exercise, We will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data. \n",
    "\n",
    "As in the previous exercise, We will use a hold-out set to evaluate your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned ElasticNet l1 ratio: {'l1_ratio': 0.0}\n",
      "Tuned ElasticNet R squared: 0.24765337510702687\n",
      "Tuned ElasticNet MSE: 0.16664179543611013\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "# Use Gapminder\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv = 5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
